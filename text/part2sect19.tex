%!TEX root = ../var.tex
\begin{zam}
\label{zam:19.1}
	Мы знаем, что для независимых случайных величин
	дисперсия их суммы равна сумме их дисперсий
		\begin{equation*}
			\D(\xi_1+\xi_2)=\D \xi_2 +\D \xi_1
		\end{equation*}

	Чему равна дисперсия суммы в общем случае?
		\begin{gather*}
			\D(\xi_1+\xi_2)=\M(\xi_1+\xi_2)^2-[\M(\xi_1+\xi_2)]^2=\\=
			\M(\xi_1^2+\xi_2^2+2 \xi_1 \xi_2)-(\M \xi_1)^2 -(\M \xi_2)^2
			-2\M\xi_2\M\xi_2=\\=
			\D \xi_1+\D \xi_2+2[\M(\xi_1\xi_2)-\M \xi_1 \M \xi_2].
		\end{gather*}
	Если случайные величины $\xi_1$ и $\xi_2$ независимы, то по теор. 17.5.6. величина $\M(\xi_1\xi_2)−\M\xi_1\M\xi_2$ равна нулю. С другой стороны, из равенства её нулю вовсе не следует их независимость (см. пример 19.7). Поэтому эту величину можно использовать как <<индикатор степени зависимости>> случайных величин $\xi_1$ и $\xi_2$.
\end{zam}

\begin{definition}
	Ковариацией $\cov(\xi_1,\xi_2)$ случайных величин $\xi_2$ и
$\xi_2$ называется число
$$\cov(\xi_1,\xi_2)=\M[(\xi_1-\M \xi_2)(\xi_2-\M \xi_2)] $$
\end{definition}

\begin{lemma}
	Свойства ковариации
	\begin{enumerate}
		\item $\cov(\xi_1,\xi_2)=\M(\xi_1 \xi_2)-\M \xi_1 \M \xi_2$,
		\item $\cov(\xi,\xi)=\D \xi	$
		\item $\cov(\xi_1,\xi_2)=\cov(\xi_2,\xi_1)$
		\item $\cov(C \xi_1,\xi_2)=C\cov(\xi_1,\xi_2)$.
	\end{enumerate}
\end{lemma}

\begin{proof}
\hspace{0pt}
	\begin{enumerate}
		\item $\cov(\xi_1,\xi_2)=\M[(\xi_1-\M \xi_1)(\xi_2-\M \xi_2)]=\xi_1 \xi_2 - \xi_1\M \xi_2- \xi_2\M \xi_1+ \M \xi_1 \M \xi_2)
		=$ \newline $=\M(\xi_1 \xi_2)-\M(\xi_1)\M(\xi_2)$
		
		\item Следует из опред. 19.1
		\item Очевидно
		\item $\cov(C \xi_1,\xi_2)=\M(\xi_1 \xi_2)-\M(C\xi_1 \xi_2)-\M(C \xi_1)\M \xi_2=C[\M(\xi_1\xi_2)-\M \xi_1 \M \xi_2]$
	\end{enumerate}
\end{proof}

Пусть $\xi_1,\dots, \xi_n$ — случайные величины. Для сокращения формул обозначим через $\sigma_{ij}$ ковариацию $\cov(\xi_i, \xi_j)$, где $i, j \in \{1, \ldots, n\}.$

\begin{lemma}
	(О дисперсии линейной комбинации)

	Для любых случайных величин $\xi_1,\dots , \xi_n$ и любых чисел $c_1,\dots, c_n \in \mathbb{R}$ справедливо
	\begin{equation*}
		\D(c_1 \xi_1+\ldots+c_n \xi_n)=\sum_{i=1}^n\sum_{j=1}^n\sigma_{ij}c_ic_j.
	\end{equation*}
\end{lemma}

\begin{proof}
	Рассмотрим случайную величину $\eta = c_1 \xi_1 +\dots+ c_n \xi_n$.
	Нетрудно увидеть, что
	\begin{equation*}
		\eta-\M\eta=\sum_{i=1}^nc_i(\xi_i-\M \xi_i),
	\end{equation*}
	и что
	\begin{equation*}
		(\eta-\M\eta)^2=\sum_{i=1}^n\sum_{j=1}^nc_ic_j(\xi_i-\M \xi_i)(\xi_j-\M \xi_j).
	\end{equation*}
	Вычисляя математическое ожидание от обеих частей последнего равенства,
	получим требуемый результат.
\end{proof}

\begin{zam}\label{zam:19.5}
	Т.к. $\D\eta \geqslant 0$, то квадратичная форма $$\sum_{i=1}^n\sum_{j=1}^n\sigma_{ij}c_ic_j,$$ где $c_1,\dots, c_n \in \mathbb{R}$ рассматриваются как переменные, является неотрицательной. Из теории квадратичных форм известно, что все главные миноры матрицы неотрицательной квадратичной формы, в нашем случае матрицы
	\begin{equation*}
		(\sigma_{ij})=	
		\begin{pmatrix}
	  		\sigma_{11}& \sigma_{12}& \ldots & \sigma_{1n}\\
	  		\sigma_{21}& \sigma_{22}& \ldots & \sigma_{2n} \\
	  		\ldots & \ldots& \ldots & \ldots \\
	  		\sigma_{n1}& \sigma_{n2}& \ldots & \sigma_{nn}\\
		\end{pmatrix}
	\end{equation*}
	неотрицательны, и в частности, $\det\sigma_{ij} \geqslant 0$. При $n = 2$ это неравенство принимает вид
	\begin{equation*}
		\begin{vmatrix}
			\sigma_{11}& \sigma_{12}\\
		 	\sigma_{21}& \sigma_{22}\\
		\end{vmatrix}
		=
		\begin{vmatrix}
			\D \xi_1 & \cov{\xi_1 \xi_2}\\
			\cov{\xi_2 \xi_1} & \D \xi_2\\
		\end{vmatrix}
		=\D \xi_1 \D \xi_2-\cov^2(\xi_1, \xi_2)\geqslant 0		
	\end{equation*}
\end{zam}

\begin{zam}\label{zam:19.6}
Обсудим достоинства и недостатки ковариации, как
величины, характеризующей зависимость двух случайных величин.
\begin{enumerate}
	\item Если ковариация $\cov(\xi_1, \xi_2)$ отлична от нуля, то величины 
	$\xi_1$ и $\xi_2$ зависимы.
	
	\item Если ковариация $\cov(\xi_1, \xi_2)$ равна нулю, то зависимость случайных
	величин $\xi_1$ и $\xi_2$ приходится исследовать с помощью пп. 17.5 и 17.6.
	
	\item Самая сильная зависимость между случайными величинами — функциональная, а из функциональных — линейная зависимость, когда $\eta =
	a \xi + b$. Встречаются более слабые зависимости. Например, если по последовательности независимых случайных величин $\xi_1, \xi_2, \dots$построить случайные величины $\xi = \xi_1 +\dots + \xi_{32} + \xi_{33}$ и $\eta = \xi_{33} + \xi_{34}+\dots + \xi_{101}$, то эти
	величины зависимы, но очень <<слабо>> и лишь через одно-единственное общее слагаемое $\xi_{33}$. (Представьте себе, сильно ли зависимы число появлений
	орла в первых 33-х подбрасываниях монеты и число появлений орла в той
	же серии, но в испытаниях с 33-го по 101-е.)

	\item Ковариация не является <<безразмерной>>. Если $\xi$ — объём газа в сосуде, а $\eta$ — давление этого газа, то ковариация $\cov(\xi_1, \xi_2)$ измеряется в $ m^3 \cdot P $. При переходе к другой размерности, т.е. при умножении одной из величин $\xi, \eta$ на какое-нибудь число, ковариация тоже умножается на это число. Но умножение на число не сказывается на <<степени зависимости>> величин $\xi$ и $\eta$. Поэтому с физической точки зрения имеет смысл сравнивать зависимость случайных величин, ковариации которых имеют одну и ту же размерность.
	Из сказанного следует, что нужно как-то нормировать ковариацию,
	чтобы получить из неё безразмерную величину, абсолютное значение которой не менялось бы при умножении или сдвиге случайных величин на число и свидетельствовало бы о <<силе>> зависимости случайных величин.
	Такую величину доставляет следующее определение.
\end{enumerate}
\end{zam}

\begin{definition}
	Коэффициентом корреляции $\rho(\xi, \eta)$ случайных величин $\xi$ и $\eta$ называется число
	\begin{equation*}
		\rho(\xi,\eta)=\frac{\cov(\xi,\eta)}{\sqrt{\D\xi}\sqrt{\D\eta}}.
	\end{equation*}
\end{definition}
\begin{zam}\label{zam:19.8}
	Чтобы увидеть <<устройство>> коэффициента корреляции, распишем по определению величины, стоящие в числителе и знаменателе
	\begin{equation*}
		\rho(\xi, \eta)=\frac{\M[(\xi-\M \xi)(\eta-\M\eta)]}
		{\sqrt{\M(\xi-\M)^2}\sqrt{\M(\eta-\M\eta)^2}}.
	\end{equation*}
		Для физиков уместно провести аналогию формулы косинуса угла между
	векторами в евклидовом пространстве и формулой коэффициента корреляции.
	
	Во-первых, так как для любых случайных величин 
	$\xi, \eta : \Omega \to \mathbb{R}$ определена их сумма 
	$(\xi +\eta)(x) = \xi(x)+\eta(x)$ и умножение любой случайной величины на число 
	$(\alpha \cdot \xi)(x) = \alpha \cdot \xi(x)$, где $\alpha \in \mathbb{R}$, то эти операции превращают
	множество $\{\xi : \Omega \to \mathbb{R}\}$ всех случайных величин в линейное (векторное)	пространство.
	
	Во-вторых, в этом пространстве ковариация $\cov(\xi, \eta)$ является скалярным произведением двух <<векторов>> $\xi$ и $\eta$ и превращает его в евклидово.

	И в-третьих, <<длиной>> случайной величины $\xi$ является её среднеквадратичное отклонение, которая равна корню из скалярного произведения <<вектора>> $\xi$ самого на себя
	$\sqrt{\cov(\xi, \xi)}=\sqrt{\D\xi}$. Поэтому коэффициент корреляции есть косинус угла между случайными величинами $\xi$ и $\eta$ в пространстве случайных величин. Надо только доказать, что $−1 \leqslant \rho(\xi, \eta) \leqslant 1$. Заметим, что т.к. пространство случайных величин бесконечномерное, то рисовать в виде векторов можно только те случайные величины, которые	в фиксированном базисе представимы в виде конечных линейных комбинаций элементов этого базиса.
\end{zam}

\begin{definition}
	Случайные величины $\xi$ и $\eta$ называются некоррелированными, если $\rho(\xi, \eta) = 0$. (Некоррелированность является аналогом
ортогональности.)
\end{definition}

\begin{theorem}[Свойства коэффициента корреляции]
	Свойства:
	\begin{enumerate}
		\item Если случайные величины $\xi$ и $\eta$ независимы, то $\rho(\xi, \eta) = cov(\xi, \eta) =0.$
		\item $|\rho(\xi, \eta)| ≤ 1$.
		\item $|\rho(\xi, \eta)| = 1$ тогда и только тогда, когда случайные величины $\xi$ и $\eta$ линейно зависимы, т.е. существуют числа $a, b \in \mathbb{R} $и $a \neq 0$, такие что $\xi = a\eta + b$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item Косвенно мы уже это доказали, см. пп. 19.5.6) и 19.3.1).
		\item Требуемое неравенство следует из неравенства $\D\xi_1\D\xi_2−\cov^2(\xi_1, \xi_2) \geqslant 0$, доказанного в п. 19.5.
		\item Докажем сначала, что из $|\rho(\xi, \eta)|$ = 1 следует, что $\xi = a\eta + b$. Перепишем опред. 19.7 в виде
		\begin{equation*}
			\rho(\xi, \eta) =\M\left(\frac{\xi-\M \xi}{\sqrt{\D \xi}}\cdot
			\frac{\eta-\M\eta}{\sqrt{\D\eta}} \right).
		\end{equation*}
		Рассмотрим сначала случай $\rho(\xi, \eta) = +1$. Воспользуемся неравенством
		$\alpha \beta\leqslant \frac{1}{2}(\alpha^2 + \beta^2)$
		\begin{gather*}
			\rho(\xi, \eta) =\M\left(
			\frac{\xi-\M \xi}{\sqrt{\D \xi}}
			\cdot
			\frac{\eta-\M\eta}{\sqrt{\D\eta}} 
			\right)
			\leqslant\\\leqslant
			\frac{1}{2}\M
			\left[ 
			\left(\frac{\xi-\M \xi}{\sqrt{\D \xi}}\right)^2
			+
			\left(\frac{\eta-\M\eta}{\sqrt{\D\eta}}\right)^2
			\right]
			=\frac{1}{2}\cdot2=1
		\end{gather*}
		Т.к $\rho(\xi, \eta) = +1$, то последнее неравенство переходит в равенство
		\begin{equation*}
			\M\left(\frac{\xi-\M \xi}{\sqrt{\D \xi}}\cdot
			\frac{\eta-\M\eta}{\sqrt{\D\eta}} \right)=
			\frac{1}{2}\M
			\left[ 
			\left(\frac{\xi-\M \xi}{\sqrt{\D \xi}}\right)^2
			+
			\left(\frac{\eta-\M\eta}{\sqrt{\D\eta}}\right)^2
			\right]
		\end{equation*}
		которое можно переписать в виде
		\begin{equation*}
			\M\left(
			\frac{\eta-\M\eta}{\sqrt{\D\eta}}-\frac{\xi-\M \xi}{\sqrt{\D\xi}} 
			\right)^2=0
		\end{equation*}
		или в виде
		\begin{equation*}
			\M\left(\xi-\frac{\sqrt{\D \xi}}{\sqrt{\D \eta}}\eta-\M \xi+
			\frac{\sqrt{\D \xi}\M\eta}{\sqrt{\D \eta}}
			\right)^2=0
		\end{equation*}
		Обозначим $a\frac{\sqrt{\D\xi}}{\sqrt{\D\eta}}$ и $b=\frac{\sqrt{\D \xi}\M\eta}{\sqrt{\D \eta}}$, получим
		\begin{equation*}
			\M(\xi-a \eta-b)^2=0
		\end{equation*}
		По определению математического ожидания равенство нулю математического ожидания неотрицательной случайной величины означает, что эта величина равна нулю, поэтому $\xi = a\eta + b$.

		В случае $\rho(\xi, \eta) = −1$ воспользуемся неравенством $\alpha\beta \geq −\frac{1}{2}(\alpha^2 + \beta^2)$ и по аналогии из неравенства
		\begin{gather*}
			\rho=\M\left(
			\frac{\xi-\M \xi}{\sqrt{\D \xi}}
			\cdot
			\frac{\eta-\M\eta}{\sqrt{\D\eta}} 
			\right)
			\leqslant\\\leqslant
			\frac{1}{2}\M
			\left[ 
			\left(\frac{\xi-\M \xi}{\sqrt{\D \xi}}\right)^2
			+
			\left[ \left(\frac{\eta-\M\eta}{\sqrt{\D\eta}}\right)^2
			\right]\right]
			=\frac{1}{2}\cdot2=-1
		\end{gather*}
		получим требуемый результат.

		Докажем теперь, что из $\eta = a\xi + b$ следует, что $|\rho	(\xi, \eta)| = 1$. Вопользуемся свойствами математического ожидания и дисперсии.
		\begin{gather*}
			\rho(\xi,a\xi+b)=
			\M
			\left(
			\frac{\xi-\M\xi}{\sqrt{\D\xi}}
			\cdot
			\frac{a\xi+b-\M (a\xi+b)}{\sqrt{\D( a\xi+b)}}
			\right)
			=\\=
			\M\left(
			\frac{\xi-\M \xi}{\sqrt{\D\xi}}\cdot
			\frac{a\xi+b-\M (a\xi+b)}{\sqrt{\D (a\xi+b)}} 
			\right)=
			\M\left(
			\frac{\xi-\M \xi}{\sqrt{\D\xi}}\cdot
			\frac{a\xi-\M (a\xi)}{\sqrt{\D (a\xi)}} 
			\right)=\\=
			a\M\left(
			\frac{(\xi-\M\xi)^2}{\sqrt{\D\xi\D(a\xi)}}
			\right)=
			\frac{a\M(\xi-\M\xi)^2}{\sqrt{a^2\D^2\xi}}
			=\frac{a}{|a|}=
			\left\{
			\begin{aligned}
				&1, &\text{ если } a>0\\
				&-1, &\text{ если } a<0	
			\end{aligned}
			\right.
		\end{gather*}
	\end{enumerate}
\end{proof}

\begin{definition}
	Если $\rho(\xi, \eta) < 0$, то случайные величины $\xi$ и $\eta$
называются отрицательно коррелированными; если $\rho(\xi, \eta) > 0$, то — положительно коррелированными.
\end{definition}
\begin{zam}\label{zam:19.12}
Смысл знака коэффициента корреляции особенно ясен
в случае $\rho(\xi, \eta) = \pm 1$. В этом случае знак $\rho$ совпадает со знаком $a$ в равенстве $\eta = a\xi + b$. Значение $\rho(\xi, \eta) = 1$ означает, что чем больше $\xi$, тем больше и $\eta$. Напротив, $\rho(\xi, \eta) = −1$ означает, что чем больше $\xi$, тем меньше и $\eta$. Аналогично можно трактовать знак коэффициента корреляции и
в случае, когда $|\rho(\xi, \eta)| < 1$, помня при этом, что зависимость случайных
величин $\xi$ и $\eta$ не линейная и может быть даже не функциональная.
\end{zam}
\begin{num}	
	Найти коэффициент корреляции между числом выпадений единицы и числом выпадений шестёрки при n подбрасываниях игральной кости.

Решение.

 Для $i = 1, 2, 3, 4, 5, 6$ обозначим через $\xi_i$ случайную величину,
равную числу выпадений грани с $i$ очками при $n$ подбрасываниях кости. Посчитаем сначала ковариацию $\cov(\xi_1, \xi_6)$ = $\M(\xi_1\xi_6) − \M\xi_1\M\xi_6$.
Каждая из случайных величин $\xi_i$ имеет биномиальное распределение и
каждое из её значений может появиться в каждом из $n$ подбрасываниях с
вероятностью $p = 1/6$, и по п. 19.9.2) $\M\xi_i = np = n/6$ и $\D\xi_i = npq = 5n/36$.

Т.к. при каждом подбрасывании выпадает какая-то грань, то $\xi_1+\dots +\xi_6 = n.$ Т.к. игральная кость симметрична, то при $i\neq j$ все математические ожидания $\M(\xi_i\xi_j)$ одинаковы. Заметим, что из той же симметрии следует, что $\M(\xi_i\xi_i)$ то же одинаковы и равны $\M^2\xi_i = \D\xi_i + (\M\xi_i)^2 =
5n/36 + n^2/36 = (n^2 + 5n)/36.$

Подсчитаем величину $\M[\xi_1(\xi_1 +\dots + \xi_6)]$ двумя способами. Во-первых,
она равна
\begin{equation*}
	\M[\xi_1(\xi_1 + \dots+ \xi_6)] = \M(\xi_1\cdot n) = n^2/6,
\end{equation*}
а во-вторых,
\begin{equation*}
	\M[\xi_1(\xi_1 +\dots + \xi_66)] = \M\xi^2_1 + 5\M(\xi_1\xi_6) = (n^2 + 5n)/36 + 5M(\xi_1\xi_6).
\end{equation*}

Отсюда $5M(\xi_1\xi_6)=n2/6 − (n2 + 5n)/36,$т.е
\begin{equation*}
\M(\xi_1\xi_6)=\frac{n^2-n}{36} 	
\end{equation*} 
Следовательно, искомый коэффициент корреляции равен
\begin{equation*}
	\rho(\xi_1,\xi_2)=\frac{\cov(\xi_1,\xi_6)}{\sqrt{\D\xi_1}\sqrt{\D\xi_6}}=
	\frac{\M(\xi_1,\xi_6)-\M(\xi_1)\M(\xi_6)}{\sqrt{\D\xi_1}\sqrt{\D\xi_6}}=
	\frac{(n^2 − n)/36 − n^2/36}{5n/36}=-\frac{1}{5}
\end{equation*}
Естественно, что коэффициент корреляции не зависит от числа подбрасываний игральной кости.
	
\end{num}